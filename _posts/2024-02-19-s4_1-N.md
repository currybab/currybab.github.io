---
layout: post
title: Efficiently Modeling Long Sequences with Structured State Spaces (1 / N)
categories: AI
tags: papers s4 mamba
mathjax: true
---

Attention 모델에 대한 대안 중 하나인 Mamba 논문을 읽기 위해서 사전 작업으로 해당 논문을 읽게 되었다. 개인적으로는 새로운 모델에 대한 가능성에 대해 항상 궁금해왔기 때문에 RWKV, Mamba 등에 대해 항상 궁금해 왔었다. 해당 논문을 통해서 다른 모델을 떠올리는 것이 얼마나 힘든일인지 잘 알 수 있게 되었다.

해당 글에서는 Introduction 부분에 대해서 정리하고자 한다.

# Introduction

해당 논문에서는 크게 두 가지를 s4 모델의 기존 모델 대비 장점으로 제시한다. 
첫번째는 long-range dependencies(LRDs)이고 두 번째는 general-purpose sequence model이다.

## 문제: long-range dependencies(LRDs)

시퀀스 모델링의 중심 문제 중 하나로 장기간 의존성(LRDs)을 효율적으로 처리하는 것이 있다고 한다. 
장기간 의존성 문제란 시퀀스 데이터를 처리할 때 나타나는 현상으로, 시퀀스 내의 멀리 떨어진 요소들 사이의 관계 또는 의존성을 모델이 캡처하기 어려운 문제를 의미한다. 
예를 들어, 자연어 처리에서 한 문장의 시작 부분에 나타나는 주어가 문장의 끝 부분에 나타나는 동사의 의미를 결정짓는 경우, 이 두 요소 사이에 장거리 의존성이 존재한다.

Real-world에서의 시계열 데이터는 수 만개의 time-step에 걸쳐 추론을 요구하는데, 수 천개의 time-step을 다루는 소수의 모델만이 있다고 한다. (아무래도 논문이 2022년 논문이기 때문에 지금의 상황과는 다를 수도 있다고 생각이 든다. 정확히는 모르겠지만 지난 주에 나온 Gemini 1.5 pro에서 1M token에서의 해당 문제를 해결한 것으로 보인다.) 
또한 [Long-Range Arena(LRA)](https://arxiv.org/pdf/2011.04006.pdf)의 벤치마크 결과는 LRD에서 현재의 시퀀스 모델들이 결과가 좋지 못하며, Path-X와 같은 특정 작업에서는 무작위 추측보다도 못한 결과를 보인다고 한다.

LRDs 문제를 해결하기 위해서 기존 모델(continuous-time models (CTMs), RNNs, CNNs, Transformers 등)에 여러 변형들이 시도되었다고 한다. 
RNN 계열에서는 vanishing gradients를 해결하기 위한 orthogonal RNN, Lipschitz RNNs이 시도되었고 CNN 계열에서는 context size의 증가를 위한 dilated convolutions, sequence 길이에 대한 이차 의존성을 줄이기 위한 다양한 Transformer들이 있었다고 한다. 
하지만 LRA나 raw audio classification에서 결과가 좋지 못했다.


## 해결법: State Space Model (SSM)을 적용

그래서 LRD 문제를 위한 해결법으로 이 논문이 가져온 방법이 state space model(ssm)을 적용하는 것이다.
제어 이론, 계산 신경과학 등 다양한 분야에서 기초적인 과학 모델로 사용되지만, 딥러닝에는 그동안 잘 적용되지는 않았다고 한다.
챗 선생님께서는 다음과 같이 말씀하셨다.

> 이는 SSM이 시간에 따른 시스템의 상태를 모델링하는 강력한 프레임워크를 제공하지만, 딥러닝의 비선형성, 고차원성, 그리고 대규모 데이터 처리와 같은 특성과 잘 맞지 않기 때문일 수 있습니다. 딥러닝 모델, 특히 신경망은 매우 유연하고 복잡한 함수를 학습할 수 있는 능력이 있지만, SSM의 전통적인 형태와는 다른 접근 방식을 필요로 합니다. 따라서, SSM을 딥러닝에 통합하려는 시도는 이론적인 적합성과 실용적인 적용 가능성의 문제를 해결해야 합니다.

논문의 주요 저자이신 Albert Gu 씨 등은 `deep SSM`이 간단한 작업에서조차 어려움을 겪지만, 연속 시간 기억 문제를 해결하기 위해 최근에 도출된 특수 상태 행렬 $A$를 갖추면 탁월한 성능을 발휘할 수 있다는 것을 보여주었다. 
해당 행렬은 HiPPO Matrix라고 부르며 Background에서 다루게 된다. 
또한 그들이 발표한 선형 상태 공간 레이어(Linear State Space Layer, LSSL)는 CTM, RNN, CNN 모델의 강점을 개념적으로 통합하며, deep SSM이 원칙적으로 LRD를 해결할 수 있음을 증명하는 개념 증명을 제공했다.
아래는 이를 요약한 사진이다.

![lssl brings](https://github.com/currybab/currybab.github.io/assets/7679722/53ffa207-f3fa-4df3-968b-c6e8004f76c9)

### LSSL의 문제점

하지만 LSSL는 연산, 메모리 문제 때문에 실제로 사용하기에 어려움이 있었다.

state의 차원을 $N$, sequence의 길이를 $L$로 표현 했을 때 
latent state를 계산할 때에 $O(N^{2}L)$ 연산량과 $O(NL)$ 메모리를 필요로 했다.- 연산량과 메모리 모두 하한으로는 $Ω(L + N)$을 가졌다.
따라서 Gu 씨 등이 제시한 ($N=256$일 경우) 합리적인 크기의 모델들에서 LSSL은 비슷한 크기의 CNN, RNN보다 더 많은 메모리를 사용하게 되었다.

이를 해결하기 위해서 LSSL를 위한 이론적인 efficient algorithms이 제안되었지만 우리는 이들이 numerically하게 불안정한 것을 보였다.(Appendix에서 증명을 한다.)
또한 특히 `special A matrix`는 선형대수학적으로 아주 `non-normal`해서 일반적인 알고리즘의 적용을 어렵게 한다고 한다.

> 여기서 non-normal하다는 것은 normal matrix가 아니라는 뜻으로 A가 normal matrix라는 것은 $A^*A = AA^*$ 임을 뜻한다. 

### Structured State Space (S4) sequence model 소개

해당 논문에서는 이전 작업에서 critical한 computational bottleneck을 해결하기 위한 방법으로 S4를 제안한다.

1. structured state matrices $A$를 reparameterize. - $A$를 low-rank 행렬들과 정규 행렬로 분해
2. 추가적으로 standard SSM을 계수 공간(coefficient space)에서 확장하는 대신에, 주파수 공간(frequency space)에서 truncated generating function을 계산해서 multipole-like evaluation로 단순화시킴.
3. 위의 두 가지 아이디어를 조합해서 low-rank term은 Woodbury identity로 수정, normal term은 안정적으로 diagonalized(대각화).
4. 최종적으로 잘 연구되어있고 안정적인 `Cauchy kernel` 연산으로 축소될 수 있었음.

결과적으로 $\tilde{O}(N + L)$ 연산량과, $O(N + L)$ 메모리 사용량을 가져서 매우 효율적으로 작동하게 되었다. 기존 모델인 LSSL과 비교해서 30배 빨라지고 메모리 사용량은 400배 줄였으며 경험적으로 LSSL보다 나아졌다고 한다.


## 추가 강점: general-purpose sequence model

또한 머신러닝의 broad한 목표로서 넓은 단위의 문제를 다루는 general한 single model의 개발이 있다. 오늘날의 모델들은 특정 도메인에 대한 문제를 다루거나, 한정된 작업을 처리하는 능력에 제한되어 있다. 이 과정에서 특수한 작업들을 필요로 하는데 `domain-specific preprocessing`, `inductive biases`, `architectures`와 같은 키워드들로 정리할 수 있겠다. sequence model들은 이러한 특수한 작업들을 줄이고 많은 문제들을 해결하는 general framework를 제공한다.

`Deep SSM`은 LRD 문제를 해결하기 위한 접근법과 continuous-time, convolution, recurrent 모델 표현 사이를 유연하게 적용할 수 있기 때문에 general sequence modeling solution에 개념적으로 강점이 있다고 하며 강점에 대해 소개한다.
   
1. Large-scale generative modeling.        
> On **CIFAR-10 density estimation**, S4 is competitive with the best autoregressive models (2.85 bits per dim). 
    
> On **WikiText-103 language modeling**, S4 substantially closes the gap to Transformers (within 0.8 perplexity), setting SoTA for attention-free models.
     
        
2. Fast autoregressive generation
            
> On **CIFAR-10** and **WikiText-103** 에서 RNN처럼, latent state를 사용해서 autoregressive model의 standard보다 60배 빠른 pixel/token generation을 수행
            
        
3. Sampling resolution change            
> Like specialized CTMs, S4는 time-series sampling frequency의 변화에 retraining 없이 적응할 수 있음 (e.g. at `0.5×` frequency on speech classification.)
            
        
4. Learning with **weaker inductive biases**      
> architectural changes없이, S4는 speech classification에서 **Speech CNNs**을 능가, time-series forecasting problems에서 **specialized Informer model**을 능가, sequential CIFAR에서 90% accuracy 이상으로 **2-D ResNet** 과 일치하는 결과

