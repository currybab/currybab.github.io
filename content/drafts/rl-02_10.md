+++
categories = ['note']
# project_url = 'https://github.com/gohugoio/hugo'
series = ['rl from scratch']
tags = ['강화학습']
title = "밑시딥 4 강화학습 내용 정리 (2/10)"
date = 2024-02-20T04:50:44+09:00
math = true
draft = true
+++

## CH02 마르코프 결정 과정

- 밴디트 문제에서는 에이전트가 어떤 행동을 취하든 다음에 도전할 문제의 설정은 변하지 않음.
- 에이전트의 행동에 따라 상태가 변하는 문제를 다룰 예정

  

- 마르코프 결정 과정 (Markov Decision Process, MDP)
    - 결정 과정: 에이전트가 (환경과 상호작용하면서) 행동을 결정하는 과정
    - 상태(state): 에이전트의 행동에 따라 에이전트가 처하는 상황
    - 타임 스텝(time step) : 에이전트가 다음 행동을 결정하는 간격
    - 에이전트는 눈앞의 보상이 아니라 미래에 얻을 수 있는 보상의 총합을 고려해야 함. 즉, 보상의 총합을 극대화하려 노력해야 함.
    - MDP의 사이클: 상태 $S_{t}$에서 행동 $A_{t}$를 수행하고 보상 $R_{t}$를 받고 다음 상태인 $S_{t+1}$로 전환.
        - 보상을 $R_{t}$로 처리할 수도 $R_{t+1}$로 처리할 수도 있음.
    - MDP는 에이전트와 환경의 상호작용을 수식으로 표현
        - 상태 전이 : 상태는 어떻게 전이되는가?
        - 보상 : 보상은 어떻게 주어지는가?
        - 정책 : 에이전트는 행동을 어떻게 결정하는가?

  

- 상태 전이
    - 결정적(deterministic) : 상태 전이가 결정적일 경우, 다음 상태 $s\prime$은 현재상태 $s$와 행동 $a$에 의해 단 하나로 결정.
        - 상태 전이 함수(state transition function) : $s\prime = f(s,a)$
    - 확률적(stochastic) : 에이전트가 왼쪽으로 이동하는 행동을 선택하더라도 0.9의 확률로만 왼쪽으로 이동하고, 0.1의 확률로는 그 자리에 머무름.
        - 외부나 내부 요인으로 인해 제대로 작동하지 않을 수 있어서...
    - 상태 전이 확률 (state transition probability)
        - 에이전트가 상태 $s$에서 행동 $a$를 선택했을 때, 다음 상태 $s\prime$으로 이동할 확률 : $p(s\prime | s,a)$
        - 다음 상태 $s\prime$를 결정하는 데 현재 상태 $s$와 행동 $a$만이 영향을 줌.
        - 다시  말해 상태 전이에서는 과거의 정보, 즉 지금까지 어떤 상태들을 거쳐 왔고 어떤 행동들을 취해 왔는지는 신경 쓰지 않음
    - 마르코프 성질(markov property) : 현재의 정보만 고려하는 성질
        - MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이(와 보상)을 모델링
        - 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야 해서, 그 조합이 기하급수적으로 많아짐.

  

- 보상 함수(reward function)
    - 보상이 결정적으로 주어진다고 가정하고 에이전트가 상태 $s$에서 행동 $a$를 수행하여 다음 상태 $s\prime$가 되었을 때 얻는 보상 : $r(s, a, s\prime)$
    - 보상이 확률적으로 주어질 수도 있음. 보상함수가 보상의 기댓값을 반환하도록 설정하면 이어지는 MDP이론(벨만 방정식 등)은 보상이 결정적일 때와 다름없이 성립함.

  

- 에이전트의 정책(policy)
    - 에이전트가 행동을 결정하는 방식
    - 에이전트는 ‘현재 상태’만으로 행동을 결정할 수 있음
        - 충분한 이유 ? 환경의 상태 전이가 마르코프 성질에 따라 이루어지기 때문
        - 환경 상태의 전이는 현재 상태 $s$와 행동 $a$만을 고려하여 다음 상태 $s\prime$을 결정
        - 보상도 현재 상태 $s$와 행동 $a$ 그리고 전이된 상태 $s\prime$만으로 결정.
        - → 환경에 대해 필요한 정보는 모두 현재 상태에 있다.
        - 따라서 현재 상태만으로 행동을 결정할 수 있음.
    - 결정적 정책 : 각 상태($s$)에 따라 반드시 정해진 행동($\alpha$)을 함.   $\alpha = \mu(s)$
    - 확률적 정책 : 상태($s$)에서 행동 $a$를 취할 확률.    $\pi(a|s)$  
  

> MDP의 마르코프 성질이라는 특성은 에이전트에 대한 제약이 아니라 환경에 대한 제약으로 볼 수 있음.  
> 즉, 마르코프 성질을 만족하도록 상태를 관리하는 책임이 환경쪽에 있다는 뜻.

  

- MDP의 목표
    - 에이전트는 정책 $\pi(a | s)$에 따라 행동하고 상태 전이 확률 $p(s\prime | s,a)$에 의해 다음 상태가 결정됨. 보상은 보상 함수 $r(s,a,s\prime)$가 결정.
    - 이 틀 안에서 최적 정책을 찾는 것이 MDP의 목표
    - 최적 정책 (optimal policy) : 수익이 최대가 되는 정책
    - MDP의 문제는 크게 두가지로 나뉨 → (1) 일회성 과제, (2) 지속적 과제
    - 일회성 과제 (episodic task)

        - ‘끝’이 있는 문제
        - 에피소드(episode) : 시작부터 끝까지의 일련의 시도
    - 지속적 과제 (continuous task)
        - ‘끝’이 없는 문제
        - 재고 관리 문제 등
    - 수익(return) : 에이전트가 얻는 보상의 합, 수익을 극대화 하는 것이 에이전트의 목표
        - 시간 $t$에서의 상태를 $S_{t}$라고 하자. 그리고 에이전트가 정책 $\pi$에 따라 행동 $A _ {t}$를 하고, 보상 $R_t$를 얻고 새로운 상태 $S_{t+1}$로 전이하는 흐름이 이어짐.
        - 이때 수익 $G_{t} = R_{t} + \gamma R_{t+1} + \gamma^{2} R_{t+2} + ...$ 과 같이 정의됨.
        - 할인율 (discount rate, $\gamma$) : 시간이 지날수록 보상이 줄어드는 비율
            - 할인율을 도입하는 주된 이유는 지속적 과제에서 수익이 무한대가 되지 않도록 방지하기 위해서임.
            - 또한 가까운 미래의 보상을 더 중요하게 보이도록 함.

    - 상태 가치 함수(state-value function) : 수익의 기댓값
        - 에이전트와 환경이 ‘확률적’으로 동작할 수 있음 → 같은 상태에서 시작하더라도 수익이 에피소드마다 확률적으로 달라짐.
        - 확률적 동작에 대응하기 위해서는 기댓값, 즉 ‘수익의 기댓값’을 지표로 삼아야 함.
        - 상태 $S_{t}$가 $s$이고, 에이전트의 정책이 $\pi$일 때, 에이전트가 얻을 수 있는 기대 수익  
상태 가치 함수(state-value function)  :  $$v_{\pi}(s)= \mathbb{E}[G_{t}|S_{t} = s,\pi] = \mathbb{E_{\pi}}[G_{t}|S_{t} = s] $$
        - 위 함수에서 에이전트의 정책 $\pi$는 조건으로 주어짐.정책 $\pi$가 바뀌면 에이전트가 얻는 보상도 바뀌고 수익도 바뀌기 때문에 명시하기 위해 상태 가치 함수는 $v_{\pi}(s)$처럼 $\pi$를 $v$의 밑으로 쓰는 방식을 많이 따름.

- 최적 정책
    - 두 정책의 우열을 가리려면 하나의 정책이 다른 정책보다 ‘모든 상태’에서 더 좋거나 최소한 똑같아야 함.
    - 최적 정책 $\pi_{\ast}$ 는 다른 정책과 비교하여 모든 상태에서 상태 가치 함수 $v_{\pi_{\ast}}(s)$ 값이 더 큰 정책임.
    - 결정적 정책 : MDP에서는 최적 정책이 적어도 하나는 존재하고 그 최적 정책은 ‘결정적 정책’이다.
        - 결정적 정책에서는 각 상태에서의 행동이 유일하게 결정 됨.
        - 수식으로는 $\alpha = \mu_{\ast}(s)$와 같이, 상태 $s$를 입력하면 행동 $\alpha$를 출력하는 함수 $\mu_{\ast}$로 나타낼 수 있음.
    - 최적 상태 가치 함수(optimal state-value function, $v_{\ast}$) : 최적 정책의 상태 가치 함수

  

- 백업 다이어 그램(backup diagram) : 방향 있는 그래프(노드와 화살표로 구성된 그래프)를 활용하여 상태, 행동, 보상의 전이를 표현한 그래프