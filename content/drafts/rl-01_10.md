+++
categories = ['note']
# project_url = 'https://github.com/gohugoio/hugo'
series = ['rl from scratch']
tags = ['강화학습']
title = "밑시딥 4 강화학습 내용 정리 (1/10)"
date = 2024-02-20T02:50:44+09:00
math = true
draft = true
+++

## CH01 밴디트 문제

- 지도 학습 : 입력과 출력이 쌍으로 존재하는 데이터를 이용하여 입력을 적합한 출력으로 변환하는 학습
- 비지도 학습 : 정답 레이블이 없는 데이터를 이용하여 데이터에 숨어 있는 구조 학습

- 강화 학습 : 환경과 상호작용하며 더 나은 해결책을 스스로 학습하는 것
- 에이전트 : 행동 주체  
![](Files/%E1%84%83%E1%85%A1%E1%84%8B%E1%85%AE%E1%86%AB%E1%84%85%E1%85%A9%E1%84%83%E1%85%B3.png)  

    - 에이전트는 어떤 환경(environment)에 놓여져 환경의 상태(state)를 관찰하고, 상태에 적합한 행동(action)을 취함
    - 행동을 취한 결과로 환경의 상태가 변화함.
    - 그리고 에이전트는 환경으로부터 보상(reward)을 받음과 동시에 변화된 ‘새로운 상태’를 관찰함.
    - 강화학습의 목표는 에이전트가 얻어가는 보상의 총합을 극대화하는 행동 패턴을 익히는 것.

  

- 밴디트 문제

    - 멀티-암드 밴디트 문제(multi-armed bandit problem)
    - 슬롯머신 = 환경, 플레이어 = 에이전트
    - 행동 = 플레이어가 여러 슬롯머신 중 한대를 선택해 플레이
    - 보상 = 행동의 결과로 얻는 코인

  

- 기댓값(expectation value)
    - 수없이 많이 플레이하면 평균적으로 얻게 되는 코인 갯수는 하나의 값으로 수렴
    - 이 값을 기준으로 더 큰 쪽이 더 좋은 슬롯머신이라고 판단함.
    - 가치(value) : 보상의 기댓값
    - 행동 가치(action value) : 행동의 결과로 얻는 보상의 기댓값
    - 슬롯머신 a를 플레이하여 얻는 코인 개수의 기댓값은 ‘슬롯머신의 가치’ 또는 ‘a의 행동 가치’라고 할 수 있음.
- 보상(reward, $R$)
    - 밴디트 문제에서는 슬롯머신이 돌려주는 코인 갯수
- 행동(action, $A$)
    - 에이전트가 수행하는 행동.
    - 슬롯 머신 a와 b를 선택하는 행동을 각각 $a$와 $b$라고 한다면 변수 $A$는 $\{a, b\}$ 중 하나의 값을 취하게 됨.
- 기댓값(expectation, $\mathbb{E}$)
    - 보상 $R$의 기댓값: $\mathbb{E}[R]$
    - 행동 $A$를 선택했을 때의 보상 기댓값: $\mathbb{E}[R|A]$
    - a 행동을 선택했을 때의 보상 기댓값: $\mathbb{E}[R|A=a]$ or $\mathbb{E}[R|a]$
- 행동 가치(quality, $Q$ or $q$)
    - 행동 $A$의 행동 가치 : $q(A) = \mathbb{E}[R|A]$
    - 소문자일 때는 실제 행동 가치를 뜻하며 대문자일 때는 추정치를 뜻함.
    - 에이전트는 실제 행동가치 q(A)를 알 수 없기 때문에 그값을 추정해야 함. 이런 경우 행동 가치를 추정치라는 뜻에서 Q(A)로 표기함

  

- $\epsilon$-탐욕 정책 : $\epsilon$의 확률로 탐색을 하고 나머지는 활용을 하는 방식
    - 탐색할 차례에서는 다음 행동을 무작위로 선택하여 다양한 경험을 쌓고 나머지 확률로는 탐욕 행동(활용)을 함

  

- 정상 문제(stationary problem)
    - 보상의 확률 분포가 변하지 않는 문제
    - 밴디트 문제 맥락에서는 슬롯머신에 설정된 승률(슬롯머신의가치)은 고정되어 있음.
- 비정상 문제(non-stationary problem)
    - 보상의 확률 분포가 변하도록 설정된 문제
    - 플레이할 때마다 슬롯머신의 가치(승률)가 달라짐.
    - 여기서 확률 분포는 시간에 따라서만 변함
- 기존에는 보상에 대한 가중치를 똑같이 부여했음. 하지만 비정상 문제에서는 적합하지 않음.
    - 시간이 흐르면 환경(슬롯머신)이 변하기 때문에 과거 데이터(보상)의 중요도는 점점 낮아져야 함.
    - 반대로 새로 얻은 보상의 가중치는 점점 커져야 함.
    - 정상 문제에서의 표본 평균: $Q_{n} = Q_{n-1} + \frac{1}{n} (R_{n} - Q_{n-1})$
    - 비정상 문제에서 고정 가중치를 준 행동 추정 가중치$$\begin{aligned} Q_{n} & = Q_{n-1} + \alpha (R_{n} - Q_{n-1})\\ & =\alpha R_{n} + \alpha (1 - \alpha) R_{n-1} + ... + \alpha (1 - \alpha)^{n-1} R_{1} + (1-\alpha)^{n}Q_{0} \end{aligned}$$
        - 가중 치가 기하급수적으로 감소함
        - 지수적으로 감소하기 때문에 지수 이동 평균(exponential moving average) 또는 지수 가중 이동 평균(exponential weighted moving average)라고 함.